{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs=np.array([[0.7,0.1,0.2],[0.1,0.5,0.4],[0.02,0.9,0.08]])\n",
    "class_targets=[0,1,1]\n",
    "print(softmax_outputs[[0,1,2],class_targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n",
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "# Applying cross entropy loss\n",
    "print(-np.log(softmax_outputs[range(len(softmax_outputs)),class_targets]))\n",
    "neg_log=-np.log(softmax_outputs[range(len(softmax_outputs)),class_targets])\n",
    "average_loss=np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n",
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "# If data is one hot encoded\n",
    "y_true_check=np.array([[1,0,0],[0,1,0],[0,1,0]])\n",
    "y_pred_clipped_check=np.array([[0.7,0.2,0.1],[0.1,0.5,0.4],[0.02,0.9,0.08]])\n",
    "\n",
    "A=y_true_check*y_pred_clipped_check\n",
    "B=np.sum(A,axis=1)\n",
    "C=-np.log(B)\n",
    "print(C)\n",
    "print(np.mean(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Common Loss class\n",
    "class Loss:\n",
    "    def calculate(self, output,y):\n",
    "        sample_losses=self.forward(output,y)\n",
    "        # calculate mean loss\n",
    "        data_loss=np.mean(sample_losses)\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Loss class\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def forward(self, y_pred,y_true):\n",
    "        # number of samples in a batch\n",
    "        samples=len(y_pred)\n",
    "        y_pred_clipped=np.clip(y_pred,1e-7,1-1e-7)\n",
    "        # Categorical (1D)\n",
    "        if len(y_true.shape)==1:\n",
    "            correct_confidence=y_pred_clipped[range(samples),y_true]\n",
    "\n",
    "        # for one hot encoded (2D)\n",
    "        elif len(y_true.shape)==2:\n",
    "            correct_confidence=np.sum(y_pred_clipped*y_true,axis=1)\n",
    "\n",
    "        negative_log_likelihood=-np.log(correct_confidence)\n",
    "        return negative_log_likelihood\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs=np.array([[0.7,0.1,0.2],[0.1,0.5,0.4],[0.02,0.9,0.08]])\n",
    "class_targets=np.array([[1,0,0],[0,1,0],[0,1,0]])\n",
    "loss_function=Loss_CategoricalCrossEntropy()\n",
    "loss=loss_function.calculate(softmax_outputs,class_targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the previous codes\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights=0.01*np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases=np.zeros((1,n_neurons))\n",
    "        \n",
    "\n",
    "    ## Forward pass\n",
    "    def forward(self, inputs):\n",
    "        ### Calculate output values from inputs, weights and biases\n",
    "        self.output=np.dot(inputs,self.weights)+self.biases\n",
    "\n",
    "\n",
    "### ACTIVATION FUNCTION: RELU\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self,inputs):\n",
    "        # Calculate output values from input\n",
    "        self.output=np.maximum(0,inputs)\n",
    "\n",
    "\n",
    "\n",
    "#Softmax activation\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # get unnormalised probabilities\n",
    "        exp_values=np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
    "        # Normalise them for each sample\n",
    "        probablitiles=exp_values/np.sum(exp_values,axis=1,keepdims=True)\n",
    "        self.output=probablitiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333323 0.33333333 0.33333343]\n",
      " [0.33333297 0.33333358 0.33333345]\n",
      " [0.33333297 0.33333341 0.33333363]\n",
      " [0.33333275 0.33333355 0.33333369]]\n",
      "loss: 1.098610444420201\n",
      "acc: 0.37333333333333335\n"
     ]
    }
   ],
   "source": [
    "### NEURAL NETWORK FORWARD PASS (with LOSS)\n",
    "from nnfs.datasets import spiral_data\n",
    "# Create dataset\n",
    "X,y=spiral_data(samples=100,classes=3)\n",
    "# create dense layer with 2 input features and 3 output values\n",
    "dense1=Layer_Dense(2,3)\n",
    "# create Relu activation\n",
    "activation1=Activation_ReLU()\n",
    "\n",
    "#Create second dense layer with 3 input features\n",
    "dense2=Layer_Dense(3,3) # 3-inputs and 3-neurons\n",
    "activation2=Activation_Softmax()\n",
    "\n",
    "## create loss function\n",
    "loss_function=Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# make a forward pass of our training data through this layer\n",
    "dense1.forward(X) # generate outputs from 1st layer of neuron\n",
    "\n",
    "## Make  a forward pass through activation function\n",
    "#it takes output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "# forward pass through second dense layer\n",
    "# it takes output of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output) # use output of previous activation as input\n",
    "## Make  a forward pass through activation function\n",
    "#it takes output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "# lets see output of first few samples:\n",
    "print(activation2.output[:5])\n",
    "\n",
    "loss=loss_function.calculate(activation2.output,y)\n",
    "print('loss:',loss)\n",
    "\n",
    "\n",
    "### calculate accuracy from outputs of activation2 and targets\n",
    "predictions=np.argmax(activation2.output,axis=1)\n",
    "if len(y.shape)==2:\n",
    "    y=np.argmax(y,axis=1)\n",
    "\n",
    "accurcay=np.mean(predictions==y)\n",
    "print('acc:',accurcay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "### Inroducing Accurcay\n",
    "softmax_outputs=np.array([[0.7,0.2,0.1],[0.1,0.5,0.4],[0.02,0.9,0.08]])\n",
    "class_targets=np.array([[0,1,1]])\n",
    "\n",
    "predictions=np.argmax(softmax_outputs,axis=1)\n",
    "if len(class_targets.shape)==2:\n",
    "    class_targets=np.argmax(class_targets,axis=1)\n",
    "\n",
    "accurcay=np.mean(predictions==class_targets)\n",
    "print('acc:',accurcay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
